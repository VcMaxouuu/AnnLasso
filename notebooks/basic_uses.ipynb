{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from harderLASSO.models import harderLASSORegressor, harderLASSOClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0: Total Loss = 862.34222, Mean Bare Loss = 12.31917\n",
      "\tEpoch 50: Total Loss = 269.46652, Mean Bare Loss = 3.84952\n",
      "\tEpoch 100: Total Loss = 29.76236, Mean Bare Loss = 0.42518\n",
      "\tEpoch 150: Total Loss = 6.56632, Mean Bare Loss = 0.09380\n",
      "\tEpoch 200: Total Loss = 3.40329, Mean Bare Loss = 0.04862\n",
      "\t\tConverged after 233 epochs. Penalized loss is small.\n",
      "\n",
      "### Intermediate phase 1: Lambda = 0.9655, Nu = 1 ###\n",
      "\tEpoch 0: Total Loss = 21575.37305, Mean Bare Loss = 0.00068\n",
      "\tEpoch 50: Total Loss = 20440.93555, Mean Bare Loss = 0.10960\n",
      "\tEpoch 100: Total Loss = 19336.18359, Mean Bare Loss = 0.10846\n",
      "\tEpoch 150: Total Loss = 18267.24023, Mean Bare Loss = 0.10681\n",
      "\tEpoch 200: Total Loss = 17238.21094, Mean Bare Loss = 0.10292\n",
      "\tEpoch 250: Total Loss = 16245.92578, Mean Bare Loss = 0.09873\n",
      "\tEpoch 300: Total Loss = 15286.14551, Mean Bare Loss = 0.09076\n",
      "\tEpoch 350: Total Loss = 14362.80762, Mean Bare Loss = 0.04714\n",
      "\tEpoch 400: Total Loss = 13482.64062, Mean Bare Loss = 0.04446\n",
      "\tEpoch 450: Total Loss = 12647.33105, Mean Bare Loss = 0.04281\n",
      "\tEpoch 500: Total Loss = 11847.97266, Mean Bare Loss = 0.04124\n",
      "\tEpoch 550: Total Loss = 11083.03906, Mean Bare Loss = 0.03877\n",
      "\tEpoch 600: Total Loss = 10355.30762, Mean Bare Loss = 0.03676\n",
      "\tEpoch 650: Total Loss = 9661.72266, Mean Bare Loss = 0.03547\n",
      "\tEpoch 700: Total Loss = 8999.27344, Mean Bare Loss = 0.03395\n",
      "\tEpoch 750: Total Loss = 8369.49707, Mean Bare Loss = 0.03106\n",
      "\tEpoch 800: Total Loss = 7773.23047, Mean Bare Loss = 0.03019\n",
      "\tEpoch 850: Total Loss = 7210.64795, Mean Bare Loss = 0.02812\n",
      "\tEpoch 900: Total Loss = 6682.56494, Mean Bare Loss = 0.02539\n",
      "\tEpoch 950: Total Loss = 6188.32227, Mean Bare Loss = 0.02413\n",
      "\tEpoch 1000: Total Loss = 5726.60596, Mean Bare Loss = 0.02182\n",
      "\tEpoch 1050: Total Loss = 5291.89404, Mean Bare Loss = 0.02002\n",
      "\tEpoch 1100: Total Loss = 4880.73340, Mean Bare Loss = 0.01751\n",
      "\tEpoch 1150: Total Loss = 4496.13721, Mean Bare Loss = 0.01677\n",
      "\tEpoch 1200: Total Loss = 4141.14844, Mean Bare Loss = 0.02010\n",
      "\tEpoch 1250: Total Loss = 3810.70508, Mean Bare Loss = 0.02149\n",
      "\tEpoch 1300: Total Loss = 3502.61011, Mean Bare Loss = 0.03329\n",
      "\tEpoch 1350: Total Loss = 3214.63501, Mean Bare Loss = 0.04398\n",
      "\tEpoch 1400: Total Loss = 2948.96509, Mean Bare Loss = 0.04439\n",
      "\tEpoch 1450: Total Loss = 2704.27197, Mean Bare Loss = 0.04692\n",
      "\tEpoch 1500: Total Loss = 2476.80688, Mean Bare Loss = 0.05013\n",
      "\tEpoch 1550: Total Loss = 2267.53442, Mean Bare Loss = 0.05595\n",
      "\tEpoch 1600: Total Loss = 2074.17236, Mean Bare Loss = 0.06239\n",
      "\tEpoch 1650: Total Loss = 1894.51514, Mean Bare Loss = 0.05642\n",
      "\tEpoch 1700: Total Loss = 1729.18872, Mean Bare Loss = 0.05657\n",
      "\tEpoch 1750: Total Loss = 1576.08203, Mean Bare Loss = 0.04988\n",
      "\tEpoch 1800: Total Loss = 1436.81238, Mean Bare Loss = 0.05155\n",
      "\tEpoch 1850: Total Loss = 1310.14111, Mean Bare Loss = 0.06213\n",
      "\tEpoch 1900: Total Loss = 1194.16882, Mean Bare Loss = 0.07595\n",
      "\tEpoch 1950: Total Loss = 1086.69580, Mean Bare Loss = 0.09340\n",
      "\tEpoch 2000: Total Loss = 987.73718, Mean Bare Loss = 0.11066\n",
      "\tEpoch 2050: Total Loss = 896.04358, Mean Bare Loss = 0.12257\n",
      "\tEpoch 2100: Total Loss = 811.06305, Mean Bare Loss = 0.13110\n",
      "\tEpoch 2150: Total Loss = 734.21429, Mean Bare Loss = 0.13305\n",
      "\tEpoch 2200: Total Loss = 663.50140, Mean Bare Loss = 0.12897\n",
      "\tEpoch 2250: Total Loss = 598.06946, Mean Bare Loss = 0.11408\n",
      "\tEpoch 2300: Total Loss = 539.35858, Mean Bare Loss = 0.10264\n",
      "\tEpoch 2350: Total Loss = 486.94302, Mean Bare Loss = 0.09390\n",
      "\tEpoch 2400: Total Loss = 441.45877, Mean Bare Loss = 0.09004\n",
      "\tEpoch 2450: Total Loss = 400.58850, Mean Bare Loss = 0.08636\n",
      "\tEpoch 2500: Total Loss = 364.13177, Mean Bare Loss = 0.08817\n",
      "\tEpoch 2550: Total Loss = 331.77502, Mean Bare Loss = 0.09424\n",
      "\tEpoch 2600: Total Loss = 302.99609, Mean Bare Loss = 0.09573\n",
      "\tEpoch 2650: Total Loss = 277.56857, Mean Bare Loss = 0.08772\n",
      "\tEpoch 2700: Total Loss = 255.30438, Mean Bare Loss = 0.07686\n",
      "\tEpoch 2750: Total Loss = 234.65436, Mean Bare Loss = 0.06763\n",
      "\tEpoch 2800: Total Loss = 216.46242, Mean Bare Loss = 0.06700\n",
      "\tEpoch 2850: Total Loss = 200.79861, Mean Bare Loss = 0.06978\n",
      "\tEpoch 2900: Total Loss = 186.54819, Mean Bare Loss = 0.07691\n",
      "\tEpoch 2950: Total Loss = 174.31380, Mean Bare Loss = 0.08178\n",
      "\tEpoch 3000: Total Loss = 162.55449, Mean Bare Loss = 0.07281\n",
      "\tEpoch 3050: Total Loss = 150.63368, Mean Bare Loss = 0.03385\n",
      "\tEpoch 3100: Total Loss = 140.14400, Mean Bare Loss = 0.01684\n",
      "\tEpoch 3150: Total Loss = 130.96124, Mean Bare Loss = 0.01799\n",
      "\tEpoch 3200: Total Loss = 122.09776, Mean Bare Loss = 0.00441\n",
      "\tEpoch 3250: Total Loss = 114.08399, Mean Bare Loss = 0.00454\n",
      "\tEpoch 3300: Total Loss = 106.44382, Mean Bare Loss = 0.00433\n",
      "\tEpoch 3350: Total Loss = 99.28638, Mean Bare Loss = 0.00379\n",
      "\tEpoch 3400: Total Loss = 93.20675, Mean Bare Loss = 0.00381\n",
      "\tEpoch 3450: Total Loss = 87.42165, Mean Bare Loss = 0.00432\n",
      "\tEpoch 3500: Total Loss = 82.08180, Mean Bare Loss = 0.00347\n",
      "\tEpoch 3550: Total Loss = 77.35318, Mean Bare Loss = 0.00367\n",
      "\tEpoch 3600: Total Loss = 73.44318, Mean Bare Loss = 0.00355\n",
      "\tEpoch 3650: Total Loss = 70.01037, Mean Bare Loss = 0.00392\n",
      "\tEpoch 3700: Total Loss = 66.70899, Mean Bare Loss = 0.00287\n",
      "\tEpoch 3750: Total Loss = 63.75667, Mean Bare Loss = 0.00333\n",
      "\tEpoch 3800: Total Loss = 61.26775, Mean Bare Loss = 0.00396\n",
      "\tEpoch 3850: Total Loss = 58.93041, Mean Bare Loss = 0.00313\n",
      "\tEpoch 3900: Total Loss = 56.99991, Mean Bare Loss = 0.00349\n",
      "\tEpoch 3950: Total Loss = 55.03817, Mean Bare Loss = 0.00275\n",
      "\tEpoch 4000: Total Loss = 53.59147, Mean Bare Loss = 0.00302\n",
      "\tEpoch 4050: Total Loss = 52.43251, Mean Bare Loss = 0.00351\n",
      "\tEpoch 4100: Total Loss = 51.19253, Mean Bare Loss = 0.00295\n",
      "\tEpoch 4150: Total Loss = 49.95527, Mean Bare Loss = 0.00274\n",
      "\tEpoch 4200: Total Loss = 49.09612, Mean Bare Loss = 0.00287\n",
      "\tEpoch 4250: Total Loss = 48.04910, Mean Bare Loss = 0.00272\n",
      "\tEpoch 4300: Total Loss = 47.11095, Mean Bare Loss = 0.00283\n",
      "\tEpoch 4350: Total Loss = 46.17896, Mean Bare Loss = 0.00288\n",
      "\tEpoch 4400: Total Loss = 45.20129, Mean Bare Loss = 0.00276\n",
      "\t\tConverged after 4424 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Intermediate phase 2: Lambda = 1.7950, Nu = 0.7 ###\n",
      "\tEpoch 0: Total Loss = 73.96984, Mean Bare Loss = 0.00272\n",
      "\tEpoch 50: Total Loss = 67.59586, Mean Bare Loss = 0.06158\n",
      "\tEpoch 100: Total Loss = 66.37315, Mean Bare Loss = 0.07509\n",
      "\tEpoch 150: Total Loss = 65.77914, Mean Bare Loss = 0.08160\n",
      "\tEpoch 200: Total Loss = 65.33023, Mean Bare Loss = 0.08377\n",
      "\tEpoch 250: Total Loss = 63.74019, Mean Bare Loss = 0.08487\n",
      "\tEpoch 300: Total Loss = 63.36513, Mean Bare Loss = 0.08578\n",
      "\tEpoch 350: Total Loss = 63.14814, Mean Bare Loss = 0.08632\n",
      "\tEpoch 400: Total Loss = 62.33244, Mean Bare Loss = 0.08708\n",
      "\tEpoch 450: Total Loss = 62.20234, Mean Bare Loss = 0.08773\n",
      "\tEpoch 500: Total Loss = 61.82635, Mean Bare Loss = 0.08824\n",
      "\tEpoch 550: Total Loss = 61.64896, Mean Bare Loss = 0.08876\n",
      "\t\tConverged after 558 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Intermediate phase 3: Lambda = 2.6244, Nu = 0.4 ###\n",
      "\tEpoch 0: Total Loss = 69.26442, Mean Bare Loss = 0.08883\n",
      "\tEpoch 50: Total Loss = 68.85603, Mean Bare Loss = 0.11371\n",
      "\tEpoch 100: Total Loss = 67.47810, Mean Bare Loss = 0.11415\n",
      "\tEpoch 150: Total Loss = 66.87082, Mean Bare Loss = 0.11442\n",
      "\tEpoch 200: Total Loss = 66.58768, Mean Bare Loss = 0.11457\n",
      "\t\tConverged after 203 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Intermediate phase 4: Lambda = 3.1620, Nu = 0.3 ###\n",
      "\tEpoch 0: Total Loss = 72.07386, Mean Bare Loss = 0.11458\n",
      "\tEpoch 50: Total Loss = 72.67679, Mean Bare Loss = 0.12423\n",
      "\tEpoch 100: Total Loss = 71.11324, Mean Bare Loss = 0.12460\n",
      "\tEpoch 150: Total Loss = 70.88128, Mean Bare Loss = 0.12471\n",
      "\tEpoch 200: Total Loss = 70.75665, Mean Bare Loss = 0.12476\n",
      "\t\tConverged after 217 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Intermediate phase 5: Lambda = 3.4197, Nu = 0.2 ###\n",
      "\tEpoch 0: Total Loss = 69.29326, Mean Bare Loss = 0.12477\n",
      "\tEpoch 50: Total Loss = 73.56281, Mean Bare Loss = 0.12447\n",
      "\tEpoch 100: Total Loss = 69.13406, Mean Bare Loss = 0.12476\n",
      "\t\tConverged after 127 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Final ISTA Phase ###\n",
      "\t Epoch 0: Loss = 65.60683, Mean Bare Loss = 0.12460, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 50: Loss = 65.29359, Mean Bare Loss = 0.11974, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 100: Loss = 65.03200, Mean Bare Loss = 0.11735, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 150: Loss = 64.78117, Mean Bare Loss = 0.11582, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 200: Loss = 64.50207, Mean Bare Loss = 0.11499, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 250: Loss = 64.12382, Mean Bare Loss = 0.11534, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 300: Loss = 63.37867, Mean Bare Loss = 0.11661, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 350: Loss = 63.11710, Mean Bare Loss = 0.11356, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 400: Loss = 62.93880, Mean Bare Loss = 0.11211, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 450: Loss = 62.78700, Mean Bare Loss = 0.11129, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 500: Loss = 62.64836, Mean Bare Loss = 0.11092, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 550: Loss = 62.51613, Mean Bare Loss = 0.11102, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 600: Loss = 62.37426, Mean Bare Loss = 0.11177, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 650: Loss = 62.16966, Mean Bare Loss = 0.11385, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 700: Loss = 61.55869, Mean Bare Loss = 0.11864, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 750: Loss = 61.42424, Mean Bare Loss = 0.11713, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 800: Loss = 61.30159, Mean Bare Loss = 0.11631, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 850: Loss = 61.17562, Mean Bare Loss = 0.11562, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 900: Loss = 61.04217, Mean Bare Loss = 0.11506, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 950: Loss = 60.89552, Mean Bare Loss = 0.11459, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1000: Loss = 60.72954, Mean Bare Loss = 0.11426, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1050: Loss = 60.53165, Mean Bare Loss = 0.11410, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1100: Loss = 60.27850, Mean Bare Loss = 0.11421, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1150: Loss = 59.89081, Mean Bare Loss = 0.11485, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1200: Loss = 58.75778, Mean Bare Loss = 0.11772, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1250: Loss = 58.41834, Mean Bare Loss = 0.11537, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1300: Loss = 58.10511, Mean Bare Loss = 0.11559, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1350: Loss = 57.49492, Mean Bare Loss = 0.11753, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1400: Loss = 56.79375, Mean Bare Loss = 0.11729, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1450: Loss = 56.58846, Mean Bare Loss = 0.11659, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1500: Loss = 56.32372, Mean Bare Loss = 0.11608, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1550: Loss = 55.89323, Mean Bare Loss = 0.11560, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1600: Loss = 54.54265, Mean Bare Loss = 0.11515, Important Features = (12, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 42, 137])\n",
      "\t Epoch 1650: Loss = 54.12188, Mean Bare Loss = 0.11457, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 1700: Loss = 54.07787, Mean Bare Loss = 0.11395, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 1750: Loss = 54.03073, Mean Bare Loss = 0.11328, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 1800: Loss = 53.98089, Mean Bare Loss = 0.11258, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 1850: Loss = 53.92758, Mean Bare Loss = 0.11185, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 1900: Loss = 53.86956, Mean Bare Loss = 0.11107, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 1950: Loss = 53.80786, Mean Bare Loss = 0.11026, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2000: Loss = 53.74454, Mean Bare Loss = 0.10946, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2050: Loss = 53.67953, Mean Bare Loss = 0.10868, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2100: Loss = 53.61281, Mean Bare Loss = 0.10793, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2150: Loss = 53.54430, Mean Bare Loss = 0.10720, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2200: Loss = 53.47390, Mean Bare Loss = 0.10651, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2250: Loss = 53.40142, Mean Bare Loss = 0.10585, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2300: Loss = 53.32664, Mean Bare Loss = 0.10522, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2350: Loss = 53.24921, Mean Bare Loss = 0.10463, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2400: Loss = 53.16873, Mean Bare Loss = 0.10408, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2450: Loss = 53.08461, Mean Bare Loss = 0.10357, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2500: Loss = 52.99613, Mean Bare Loss = 0.10310, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2550: Loss = 52.90224, Mean Bare Loss = 0.10267, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2600: Loss = 52.80160, Mean Bare Loss = 0.10230, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2650: Loss = 52.69239, Mean Bare Loss = 0.10199, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2700: Loss = 52.57191, Mean Bare Loss = 0.10175, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2750: Loss = 52.43613, Mean Bare Loss = 0.10160, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2800: Loss = 52.27777, Mean Bare Loss = 0.10154, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2850: Loss = 52.08352, Mean Bare Loss = 0.10164, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2900: Loss = 51.82969, Mean Bare Loss = 0.10203, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 2950: Loss = 51.45626, Mean Bare Loss = 0.10311, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 3000: Loss = 50.59947, Mean Bare Loss = 0.10693, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 3050: Loss = 50.09256, Mean Bare Loss = 0.10419, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 3100: Loss = 49.10036, Mean Bare Loss = 0.10350, Important Features = (11, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 137])\n",
      "\t Epoch 3150: Loss = 48.24699, Mean Bare Loss = 0.10308, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3200: Loss = 48.22866, Mean Bare Loss = 0.10272, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3250: Loss = 48.21289, Mean Bare Loss = 0.10241, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3300: Loss = 48.19915, Mean Bare Loss = 0.10213, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3350: Loss = 48.18710, Mean Bare Loss = 0.10189, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3400: Loss = 48.17648, Mean Bare Loss = 0.10167, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3450: Loss = 48.16710, Mean Bare Loss = 0.10147, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3500: Loss = 48.15877, Mean Bare Loss = 0.10130, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3550: Loss = 48.15083, Mean Bare Loss = 0.10113, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3600: Loss = 48.14377, Mean Bare Loss = 0.10098, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3650: Loss = 48.13734, Mean Bare Loss = 0.10084, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3700: Loss = 48.13165, Mean Bare Loss = 0.10072, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3750: Loss = 48.12642, Mean Bare Loss = 0.10061, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3800: Loss = 48.12147, Mean Bare Loss = 0.10050, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3850: Loss = 48.11683, Mean Bare Loss = 0.10041, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3900: Loss = 48.11269, Mean Bare Loss = 0.10032, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 3950: Loss = 48.10878, Mean Bare Loss = 0.10024, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 4000: Loss = 48.10514, Mean Bare Loss = 0.10016, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t Epoch 4050: Loss = 48.10192, Mean Bare Loss = 0.10009, Important Features = (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\t\tConverged after 4096 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "\tEpoch 0: Total Loss = 7.00220, Mean Bare Loss = 0.10003\n",
      "\tEpoch 50: Total Loss = 6.70310, Mean Bare Loss = 0.09576\n",
      "\tEpoch 100: Total Loss = 6.68920, Mean Bare Loss = 0.09556\n",
      "\tEpoch 150: Total Loss = 6.66627, Mean Bare Loss = 0.09523\n",
      "\tEpoch 200: Total Loss = 6.64661, Mean Bare Loss = 0.09495\n",
      "\tEpoch 250: Total Loss = 6.63868, Mean Bare Loss = 0.09484\n",
      "\tEpoch 300: Total Loss = 6.63700, Mean Bare Loss = 0.09481\n",
      "\t\tConverged after 321 epochs. Relative loss change below 1e-10.\n",
      "\n",
      "0.7932667161881275\n",
      "(10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "# Generate random training data with first s relevant features\n",
    "X = np.random.normal(size=(70, 250))\n",
    "s = 10\n",
    "features = np.arange(s)\n",
    "beta = 3*np.ones(s)\n",
    "y = X[:, features]@beta + np.random.normal(size=(X.shape[0],))\n",
    "\n",
    "# By default, model will use one hidden layer of 20 neurons and nu=0.1 for harderLASSO\n",
    "# Set hidden_dims = tuple to specify the number of hidden layers and their number of neurons\n",
    "# Set nu=None to use basic l1 LASSO\n",
    "model = harderLASSORegressor()\n",
    "model.fit(X, y, verbose = True)\n",
    "print(np.sqrt(mean_squared_error(y, model.predict(X))))\n",
    "print(model.imp_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0: Total Loss = 927.35901, Mean Bare Loss = 1.85472\n",
      "\tEpoch 50: Total Loss = 261.03406, Mean Bare Loss = 0.52207\n",
      "\tEpoch 100: Total Loss = 229.69077, Mean Bare Loss = 0.45938\n",
      "\tEpoch 150: Total Loss = 209.01532, Mean Bare Loss = 0.41803\n",
      "\tEpoch 200: Total Loss = 188.93765, Mean Bare Loss = 0.37788\n",
      "\tEpoch 250: Total Loss = 171.77992, Mean Bare Loss = 0.34356\n",
      "\tEpoch 300: Total Loss = 156.75999, Mean Bare Loss = 0.31352\n",
      "\tEpoch 350: Total Loss = 141.56624, Mean Bare Loss = 0.28313\n",
      "\tEpoch 400: Total Loss = 125.80168, Mean Bare Loss = 0.25160\n",
      "\tEpoch 450: Total Loss = 111.46611, Mean Bare Loss = 0.22293\n",
      "\tEpoch 500: Total Loss = 97.93360, Mean Bare Loss = 0.19587\n",
      "\tEpoch 550: Total Loss = 86.51194, Mean Bare Loss = 0.17302\n",
      "\tEpoch 600: Total Loss = 76.48366, Mean Bare Loss = 0.15297\n",
      "\tEpoch 650: Total Loss = 68.92387, Mean Bare Loss = 0.13785\n",
      "\tEpoch 700: Total Loss = 60.79626, Mean Bare Loss = 0.12159\n",
      "\tEpoch 750: Total Loss = 52.04725, Mean Bare Loss = 0.10409\n",
      "\tEpoch 800: Total Loss = 44.34201, Mean Bare Loss = 0.08868\n",
      "\tEpoch 850: Total Loss = 38.74031, Mean Bare Loss = 0.07748\n",
      "\tEpoch 900: Total Loss = 33.50215, Mean Bare Loss = 0.06700\n",
      "\tEpoch 950: Total Loss = 32.13460, Mean Bare Loss = 0.06427\n",
      "\tEpoch 1000: Total Loss = 22.96791, Mean Bare Loss = 0.04594\n",
      "\tEpoch 1050: Total Loss = 16.11682, Mean Bare Loss = 0.03223\n",
      "\tEpoch 1100: Total Loss = 13.76117, Mean Bare Loss = 0.02752\n",
      "\tEpoch 1150: Total Loss = 11.45609, Mean Bare Loss = 0.02291\n",
      "\tEpoch 1200: Total Loss = 10.10983, Mean Bare Loss = 0.02022\n",
      "\tEpoch 1250: Total Loss = 8.89065, Mean Bare Loss = 0.01778\n",
      "\tEpoch 1300: Total Loss = 7.80674, Mean Bare Loss = 0.01561\n",
      "\tEpoch 1350: Total Loss = 6.75196, Mean Bare Loss = 0.01350\n",
      "\tEpoch 1400: Total Loss = 5.97158, Mean Bare Loss = 0.01194\n",
      "\tEpoch 1450: Total Loss = 5.45637, Mean Bare Loss = 0.01091\n",
      "\tEpoch 1500: Total Loss = 4.95437, Mean Bare Loss = 0.00991\n",
      "\tEpoch 1550: Total Loss = 4.45470, Mean Bare Loss = 0.00891\n",
      "\tEpoch 1600: Total Loss = 3.95614, Mean Bare Loss = 0.00791\n",
      "\tEpoch 1650: Total Loss = 3.57030, Mean Bare Loss = 0.00714\n",
      "\tEpoch 1700: Total Loss = 3.28224, Mean Bare Loss = 0.00656\n",
      "\tEpoch 1750: Total Loss = 3.03621, Mean Bare Loss = 0.00607\n",
      "\tEpoch 1800: Total Loss = 2.78966, Mean Bare Loss = 0.00558\n",
      "\tEpoch 1850: Total Loss = 2.54197, Mean Bare Loss = 0.00508\n",
      "\tEpoch 1900: Total Loss = 2.29447, Mean Bare Loss = 0.00459\n",
      "\tEpoch 1950: Total Loss = 2.16335, Mean Bare Loss = 0.00433\n",
      "\tEpoch 2000: Total Loss = 1.99292, Mean Bare Loss = 0.00399\n",
      "\tEpoch 2050: Total Loss = 1.81988, Mean Bare Loss = 0.00364\n",
      "\tEpoch 2100: Total Loss = 1.65413, Mean Bare Loss = 0.00331\n",
      "\tEpoch 2150: Total Loss = 1.50538, Mean Bare Loss = 0.00301\n",
      "\tEpoch 2200: Total Loss = 1.37213, Mean Bare Loss = 0.00274\n",
      "\tEpoch 2250: Total Loss = 1.25314, Mean Bare Loss = 0.00251\n",
      "\tEpoch 2300: Total Loss = 1.14813, Mean Bare Loss = 0.00230\n",
      "\tEpoch 2350: Total Loss = 1.05594, Mean Bare Loss = 0.00211\n",
      "\t\tConverged after 2393 epochs. Relative loss change below 1e-05.\n",
      "\n",
      "### Intermediate phase 1: Lambda = 2.7837, Nu = 1 ###\n",
      "\tEpoch 0: Total Loss = 14015.42383, Mean Bare Loss = 0.00198\n",
      "\tEpoch 50: Total Loss = 13330.98730, Mean Bare Loss = 0.03251\n",
      "\tEpoch 100: Total Loss = 12662.60742, Mean Bare Loss = 0.04905\n",
      "\tEpoch 150: Total Loss = 12012.49707, Mean Bare Loss = 0.07327\n",
      "\tEpoch 200: Total Loss = 11387.73633, Mean Bare Loss = 0.09689\n",
      "\tEpoch 250: Total Loss = 10782.78613, Mean Bare Loss = 0.12064\n",
      "\tEpoch 300: Total Loss = 10200.59766, Mean Bare Loss = 0.14469\n",
      "\tEpoch 350: Total Loss = 9637.22168, Mean Bare Loss = 0.16845\n",
      "\tEpoch 400: Total Loss = 9094.32422, Mean Bare Loss = 0.19054\n",
      "\tEpoch 450: Total Loss = 8573.98047, Mean Bare Loss = 0.20983\n",
      "\tEpoch 500: Total Loss = 8075.33594, Mean Bare Loss = 0.22546\n",
      "\tEpoch 550: Total Loss = 7595.15332, Mean Bare Loss = 0.24170\n",
      "\tEpoch 600: Total Loss = 7131.06445, Mean Bare Loss = 0.25498\n",
      "\tEpoch 650: Total Loss = 6683.92529, Mean Bare Loss = 0.26834\n",
      "\tEpoch 700: Total Loss = 6256.62451, Mean Bare Loss = 0.28230\n",
      "\tEpoch 750: Total Loss = 5849.80078, Mean Bare Loss = 0.29461\n",
      "\tEpoch 800: Total Loss = 5465.83447, Mean Bare Loss = 0.30709\n",
      "\tEpoch 850: Total Loss = 5102.00879, Mean Bare Loss = 0.31943\n",
      "\tEpoch 900: Total Loss = 4758.26270, Mean Bare Loss = 0.33107\n",
      "\tEpoch 950: Total Loss = 4436.16260, Mean Bare Loss = 0.34088\n",
      "\tEpoch 1000: Total Loss = 4132.64355, Mean Bare Loss = 0.34899\n",
      "\tEpoch 1050: Total Loss = 3849.28906, Mean Bare Loss = 0.35767\n",
      "\tEpoch 1100: Total Loss = 3586.16724, Mean Bare Loss = 0.36586\n",
      "\tEpoch 1150: Total Loss = 3335.84106, Mean Bare Loss = 0.37285\n",
      "\tEpoch 1200: Total Loss = 3095.85571, Mean Bare Loss = 0.37805\n",
      "\tEpoch 1250: Total Loss = 2868.46655, Mean Bare Loss = 0.38265\n",
      "\tEpoch 1300: Total Loss = 2660.21924, Mean Bare Loss = 0.38473\n",
      "\tEpoch 1350: Total Loss = 2466.40259, Mean Bare Loss = 0.38669\n",
      "\tEpoch 1400: Total Loss = 2287.29395, Mean Bare Loss = 0.39008\n",
      "\tEpoch 1450: Total Loss = 2116.26318, Mean Bare Loss = 0.39314\n",
      "\tEpoch 1500: Total Loss = 1953.93701, Mean Bare Loss = 0.39676\n",
      "\tEpoch 1550: Total Loss = 1800.94727, Mean Bare Loss = 0.40006\n",
      "\tEpoch 1600: Total Loss = 1659.49548, Mean Bare Loss = 0.40345\n",
      "\tEpoch 1650: Total Loss = 1529.13464, Mean Bare Loss = 0.40546\n",
      "\tEpoch 1700: Total Loss = 1404.99438, Mean Bare Loss = 0.40729\n",
      "\tEpoch 1750: Total Loss = 1291.32800, Mean Bare Loss = 0.41076\n",
      "\tEpoch 1800: Total Loss = 1188.18994, Mean Bare Loss = 0.41529\n",
      "\tEpoch 1850: Total Loss = 1093.94495, Mean Bare Loss = 0.41912\n",
      "\tEpoch 1900: Total Loss = 1006.15240, Mean Bare Loss = 0.42077\n",
      "\tEpoch 1950: Total Loss = 924.73560, Mean Bare Loss = 0.42004\n",
      "\tEpoch 2000: Total Loss = 852.81946, Mean Bare Loss = 0.41661\n",
      "\tEpoch 2050: Total Loss = 787.04773, Mean Bare Loss = 0.40998\n",
      "\tEpoch 2100: Total Loss = 724.32971, Mean Bare Loss = 0.39937\n",
      "\tEpoch 2150: Total Loss = 664.53595, Mean Bare Loss = 0.38132\n",
      "\tEpoch 2200: Total Loss = 608.63220, Mean Bare Loss = 0.35948\n",
      "\tEpoch 2250: Total Loss = 561.05688, Mean Bare Loss = 0.34424\n",
      "\tEpoch 2300: Total Loss = 515.52631, Mean Bare Loss = 0.32407\n",
      "\tEpoch 2350: Total Loss = 462.34357, Mean Bare Loss = 0.27973\n",
      "\tEpoch 2400: Total Loss = 404.17624, Mean Bare Loss = 0.21821\n",
      "\tEpoch 2450: Total Loss = 357.03845, Mean Bare Loss = 0.17172\n",
      "\tEpoch 2500: Total Loss = 325.49225, Mean Bare Loss = 0.15188\n",
      "\tEpoch 2550: Total Loss = 299.40842, Mean Bare Loss = 0.14184\n",
      "\tEpoch 2600: Total Loss = 276.36981, Mean Bare Loss = 0.13594\n",
      "\tEpoch 2650: Total Loss = 255.11380, Mean Bare Loss = 0.13216\n",
      "\tEpoch 2700: Total Loss = 236.88988, Mean Bare Loss = 0.12991\n",
      "\tEpoch 2750: Total Loss = 220.72787, Mean Bare Loss = 0.12833\n",
      "\tEpoch 2800: Total Loss = 207.93816, Mean Bare Loss = 0.12709\n",
      "\tEpoch 2850: Total Loss = 198.42575, Mean Bare Loss = 0.12672\n",
      "\tEpoch 2900: Total Loss = 191.07983, Mean Bare Loss = 0.12645\n",
      "\tEpoch 2950: Total Loss = 185.85233, Mean Bare Loss = 0.12610\n",
      "\tEpoch 3000: Total Loss = 182.23834, Mean Bare Loss = 0.12570\n",
      "\tEpoch 3050: Total Loss = 178.91101, Mean Bare Loss = 0.12549\n",
      "\tEpoch 3100: Total Loss = 175.50664, Mean Bare Loss = 0.12534\n",
      "\tEpoch 3150: Total Loss = 172.75275, Mean Bare Loss = 0.12521\n",
      "\t\tConverged after 3187 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Intermediate phase 2: Lambda = 5.1753, Nu = 0.7 ###\n",
      "\tEpoch 0: Total Loss = 216.56082, Mean Bare Loss = 0.12516\n",
      "\tEpoch 50: Total Loss = 211.07683, Mean Bare Loss = 0.13107\n",
      "\tEpoch 100: Total Loss = 209.42540, Mean Bare Loss = 0.13206\n",
      "\tEpoch 150: Total Loss = 208.18906, Mean Bare Loss = 0.13250\n",
      "\tEpoch 200: Total Loss = 206.86890, Mean Bare Loss = 0.13290\n",
      "\tEpoch 250: Total Loss = 205.51256, Mean Bare Loss = 0.13348\n",
      "\t\tConverged after 284 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Intermediate phase 3: Lambda = 7.5668, Nu = 0.4 ###\n",
      "\tEpoch 0: Total Loss = 208.99254, Mean Bare Loss = 0.13426\n",
      "\tEpoch 50: Total Loss = 204.85896, Mean Bare Loss = 0.13222\n",
      "\tEpoch 100: Total Loss = 203.75154, Mean Bare Loss = 0.13150\n",
      "\tEpoch 150: Total Loss = 201.92316, Mean Bare Loss = 0.13101\n",
      "\tEpoch 200: Total Loss = 201.19543, Mean Bare Loss = 0.13073\n",
      "\tEpoch 250: Total Loss = 200.85454, Mean Bare Loss = 0.13045\n",
      "\tEpoch 300: Total Loss = 200.25473, Mean Bare Loss = 0.13020\n",
      "\t\tConverged after 336 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Intermediate phase 4: Lambda = 9.1167, Nu = 0.3 ###\n",
      "\tEpoch 0: Total Loss = 207.20178, Mean Bare Loss = 0.13002\n",
      "\tEpoch 50: Total Loss = 206.31975, Mean Bare Loss = 0.12888\n",
      "\t\tConverged after 84 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Intermediate phase 5: Lambda = 9.8597, Nu = 0.2 ###\n",
      "\tEpoch 0: Total Loss = 197.20691, Mean Bare Loss = 0.12878\n",
      "\tEpoch 50: Total Loss = 198.48163, Mean Bare Loss = 0.12740\n",
      "\tEpoch 100: Total Loss = 196.42372, Mean Bare Loss = 0.12724\n",
      "\tEpoch 150: Total Loss = 195.84866, Mean Bare Loss = 0.12718\n",
      "\t\tConverged after 164 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Final ISTA Phase ###\n",
      "\t Epoch 0: Loss = 183.76321, Mean Bare Loss = 0.12711, Important Features = (6, [0, 1, 2, 3, 6, 13])\n",
      "\t Epoch 50: Loss = 183.00043, Mean Bare Loss = 0.12607, Important Features = (6, [0, 1, 2, 3, 6, 13])\n",
      "\t Epoch 100: Loss = 182.12386, Mean Bare Loss = 0.12552, Important Features = (6, [0, 1, 2, 3, 6, 13])\n",
      "\t Epoch 150: Loss = 180.71405, Mean Bare Loss = 0.12518, Important Features = (6, [0, 1, 2, 3, 6, 13])\n",
      "\t Epoch 200: Loss = 174.28493, Mean Bare Loss = 0.12501, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 250: Loss = 174.12190, Mean Bare Loss = 0.12481, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 300: Loss = 173.96883, Mean Bare Loss = 0.12469, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 350: Loss = 173.81825, Mean Bare Loss = 0.12462, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 400: Loss = 173.65778, Mean Bare Loss = 0.12455, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 450: Loss = 173.48334, Mean Bare Loss = 0.12450, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 500: Loss = 173.28424, Mean Bare Loss = 0.12444, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 550: Loss = 173.06186, Mean Bare Loss = 0.12438, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 600: Loss = 172.79544, Mean Bare Loss = 0.12432, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 650: Loss = 172.46878, Mean Bare Loss = 0.12428, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 700: Loss = 172.03574, Mean Bare Loss = 0.12426, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 750: Loss = 171.39157, Mean Bare Loss = 0.12424, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 800: Loss = 170.11774, Mean Bare Loss = 0.12426, Important Features = (5, [0, 1, 2, 3, 6])\n",
      "\t Epoch 850: Loss = 163.90277, Mean Bare Loss = 0.12427, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 900: Loss = 163.88232, Mean Bare Loss = 0.12421, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 950: Loss = 163.86333, Mean Bare Loss = 0.12417, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1000: Loss = 163.84554, Mean Bare Loss = 0.12412, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1050: Loss = 163.82884, Mean Bare Loss = 0.12408, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1100: Loss = 163.81311, Mean Bare Loss = 0.12404, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1150: Loss = 163.79832, Mean Bare Loss = 0.12400, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1200: Loss = 163.78438, Mean Bare Loss = 0.12397, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1250: Loss = 163.77121, Mean Bare Loss = 0.12394, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1300: Loss = 163.75876, Mean Bare Loss = 0.12390, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1350: Loss = 163.74693, Mean Bare Loss = 0.12387, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1400: Loss = 163.73572, Mean Bare Loss = 0.12384, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1450: Loss = 163.72507, Mean Bare Loss = 0.12381, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1500: Loss = 163.71495, Mean Bare Loss = 0.12379, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1550: Loss = 163.70529, Mean Bare Loss = 0.12376, Important Features = (4, [0, 1, 2, 3])\n",
      "\t Epoch 1600: Loss = 163.69609, Mean Bare Loss = 0.12374, Important Features = (4, [0, 1, 2, 3])\n",
      "\t\tConverged after 1606 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "\tEpoch 0: Total Loss = 61.85063, Mean Bare Loss = 0.12370\n",
      "\tEpoch 50: Total Loss = 60.28396, Mean Bare Loss = 0.12057\n",
      "\tEpoch 100: Total Loss = 59.34160, Mean Bare Loss = 0.11868\n",
      "\tEpoch 150: Total Loss = 57.91251, Mean Bare Loss = 0.11583\n",
      "\tEpoch 200: Total Loss = 55.68094, Mean Bare Loss = 0.11136\n",
      "\tEpoch 250: Total Loss = 54.95016, Mean Bare Loss = 0.10990\n",
      "\tEpoch 300: Total Loss = 53.39750, Mean Bare Loss = 0.10680\n",
      "\tEpoch 350: Total Loss = 49.96252, Mean Bare Loss = 0.09993\n",
      "\tEpoch 400: Total Loss = 48.06657, Mean Bare Loss = 0.09613\n",
      "\tEpoch 450: Total Loss = 46.59242, Mean Bare Loss = 0.09318\n",
      "\tEpoch 500: Total Loss = 45.47334, Mean Bare Loss = 0.09095\n",
      "\tEpoch 550: Total Loss = 44.59640, Mean Bare Loss = 0.08919\n",
      "\tEpoch 600: Total Loss = 43.85619, Mean Bare Loss = 0.08771\n",
      "\tEpoch 650: Total Loss = 43.00347, Mean Bare Loss = 0.08601\n",
      "\tEpoch 700: Total Loss = 42.09209, Mean Bare Loss = 0.08418\n",
      "\tEpoch 750: Total Loss = 41.25253, Mean Bare Loss = 0.08251\n",
      "\tEpoch 800: Total Loss = 40.40536, Mean Bare Loss = 0.08081\n",
      "\tEpoch 850: Total Loss = 39.68068, Mean Bare Loss = 0.07936\n",
      "\tEpoch 900: Total Loss = 38.96772, Mean Bare Loss = 0.07794\n",
      "\tEpoch 950: Total Loss = 38.25078, Mean Bare Loss = 0.07650\n",
      "\tEpoch 1000: Total Loss = 37.42516, Mean Bare Loss = 0.07485\n",
      "\tEpoch 1050: Total Loss = 36.67995, Mean Bare Loss = 0.07336\n",
      "\tEpoch 1100: Total Loss = 36.11224, Mean Bare Loss = 0.07222\n",
      "\tEpoch 1150: Total Loss = 35.32771, Mean Bare Loss = 0.07066\n",
      "\tEpoch 1200: Total Loss = 34.59022, Mean Bare Loss = 0.06918\n",
      "\tEpoch 1250: Total Loss = 33.86573, Mean Bare Loss = 0.06773\n",
      "\tEpoch 1300: Total Loss = 33.14759, Mean Bare Loss = 0.06630\n",
      "\tEpoch 1350: Total Loss = 32.17850, Mean Bare Loss = 0.06436\n",
      "\tEpoch 1400: Total Loss = 30.70443, Mean Bare Loss = 0.06141\n",
      "\tEpoch 1450: Total Loss = 29.62529, Mean Bare Loss = 0.05925\n",
      "\tEpoch 1500: Total Loss = 28.57163, Mean Bare Loss = 0.05714\n",
      "\tEpoch 1550: Total Loss = 27.58301, Mean Bare Loss = 0.05517\n",
      "\tEpoch 1600: Total Loss = 26.71270, Mean Bare Loss = 0.05343\n",
      "\tEpoch 1650: Total Loss = 26.00152, Mean Bare Loss = 0.05200\n",
      "\tEpoch 1700: Total Loss = 25.46002, Mean Bare Loss = 0.05092\n",
      "\tEpoch 1750: Total Loss = 25.18734, Mean Bare Loss = 0.05037\n",
      "\tEpoch 1800: Total Loss = 24.93309, Mean Bare Loss = 0.04987\n",
      "\tEpoch 1850: Total Loss = 24.68360, Mean Bare Loss = 0.04937\n",
      "\tEpoch 1900: Total Loss = 24.41775, Mean Bare Loss = 0.04884\n",
      "\tEpoch 1950: Total Loss = 24.15716, Mean Bare Loss = 0.04831\n",
      "\tEpoch 2000: Total Loss = 23.93154, Mean Bare Loss = 0.04786\n",
      "\tEpoch 2050: Total Loss = 23.72412, Mean Bare Loss = 0.04745\n",
      "\tEpoch 2100: Total Loss = 23.53051, Mean Bare Loss = 0.04706\n",
      "\tEpoch 2150: Total Loss = 23.34855, Mean Bare Loss = 0.04670\n",
      "\tEpoch 2200: Total Loss = 23.18047, Mean Bare Loss = 0.04636\n",
      "\tEpoch 2250: Total Loss = 23.02456, Mean Bare Loss = 0.04605\n",
      "\tEpoch 2300: Total Loss = 22.87861, Mean Bare Loss = 0.04576\n",
      "\tEpoch 2350: Total Loss = 22.74200, Mean Bare Loss = 0.04548\n",
      "\tEpoch 2400: Total Loss = 22.61691, Mean Bare Loss = 0.04523\n",
      "\tEpoch 2450: Total Loss = 22.50272, Mean Bare Loss = 0.04501\n",
      "\tEpoch 2500: Total Loss = 22.39825, Mean Bare Loss = 0.04480\n",
      "\tEpoch 2550: Total Loss = 22.37132, Mean Bare Loss = 0.04474\n",
      "\tEpoch 2600: Total Loss = 22.28378, Mean Bare Loss = 0.04457\n",
      "\tEpoch 2650: Total Loss = 22.26191, Mean Bare Loss = 0.04452\n",
      "\tEpoch 2700: Total Loss = 22.24049, Mean Bare Loss = 0.04448\n",
      "\tEpoch 2750: Total Loss = 22.21924, Mean Bare Loss = 0.04444\n",
      "\tEpoch 2800: Total Loss = 22.19818, Mean Bare Loss = 0.04440\n",
      "\tEpoch 2850: Total Loss = 22.17731, Mean Bare Loss = 0.04435\n",
      "\tEpoch 2900: Total Loss = 22.15664, Mean Bare Loss = 0.04431\n",
      "\tEpoch 2950: Total Loss = 22.13616, Mean Bare Loss = 0.04427\n",
      "\tEpoch 3000: Total Loss = 22.11588, Mean Bare Loss = 0.04423\n",
      "\tEpoch 3050: Total Loss = 22.09581, Mean Bare Loss = 0.04419\n",
      "\tEpoch 3100: Total Loss = 22.07592, Mean Bare Loss = 0.04415\n",
      "\tEpoch 3150: Total Loss = 22.05624, Mean Bare Loss = 0.04411\n",
      "\tEpoch 3200: Total Loss = 22.03676, Mean Bare Loss = 0.04407\n",
      "\tEpoch 3250: Total Loss = 22.01750, Mean Bare Loss = 0.04403\n",
      "\tEpoch 3300: Total Loss = 21.99854, Mean Bare Loss = 0.04400\n",
      "\tEpoch 3350: Total Loss = 21.97999, Mean Bare Loss = 0.04396\n",
      "\tEpoch 3400: Total Loss = 21.96187, Mean Bare Loss = 0.04392\n",
      "\tEpoch 3450: Total Loss = 21.94417, Mean Bare Loss = 0.04389\n",
      "\tEpoch 3500: Total Loss = 21.92690, Mean Bare Loss = 0.04385\n",
      "\tEpoch 3550: Total Loss = 21.91008, Mean Bare Loss = 0.04382\n",
      "\tEpoch 3600: Total Loss = 21.89369, Mean Bare Loss = 0.04379\n",
      "\tEpoch 3650: Total Loss = 21.87773, Mean Bare Loss = 0.04376\n",
      "\tEpoch 3700: Total Loss = 21.86218, Mean Bare Loss = 0.04372\n",
      "\tEpoch 3750: Total Loss = 21.84705, Mean Bare Loss = 0.04369\n",
      "\tEpoch 3800: Total Loss = 21.83235, Mean Bare Loss = 0.04366\n",
      "\tEpoch 3850: Total Loss = 21.81804, Mean Bare Loss = 0.04364\n",
      "\tEpoch 3900: Total Loss = 21.80415, Mean Bare Loss = 0.04361\n",
      "\tEpoch 3950: Total Loss = 21.79065, Mean Bare Loss = 0.04358\n",
      "\tEpoch 4000: Total Loss = 21.77754, Mean Bare Loss = 0.04356\n",
      "\tEpoch 4050: Total Loss = 21.76483, Mean Bare Loss = 0.04353\n",
      "\tEpoch 4100: Total Loss = 21.75250, Mean Bare Loss = 0.04350\n",
      "\tEpoch 4150: Total Loss = 21.74055, Mean Bare Loss = 0.04348\n",
      "\tEpoch 4200: Total Loss = 21.72897, Mean Bare Loss = 0.04346\n",
      "\tEpoch 4250: Total Loss = 21.71778, Mean Bare Loss = 0.04344\n",
      "\tEpoch 4300: Total Loss = 21.70702, Mean Bare Loss = 0.04341\n",
      "\tEpoch 4350: Total Loss = 21.69666, Mean Bare Loss = 0.04339\n",
      "\tEpoch 4400: Total Loss = 21.68718, Mean Bare Loss = 0.04337\n",
      "\tEpoch 4450: Total Loss = 21.68550, Mean Bare Loss = 0.04337\n",
      "\t\tConverged after 4468 epochs. Relative loss change below 1e-10.\n",
      "\n",
      "0.9698008943557397\n",
      "(4, [0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(size=(500, 50))\n",
    "s = 4\n",
    "y = np.zeros(shape=(X.shape[0], ))\n",
    "for i in range(0, s, 2):\n",
    "    y+=10*np.abs(X[:, i]-X[:, i+1])\n",
    "y+= np.random.normal(size=(X.shape[0], ))\n",
    "\n",
    "model = harderLASSORegressor(hidden_dims=(20, 10))\n",
    "model.fit(X,y, verbose = True)\n",
    "print(np.sqrt(mean_squared_error(y, model.predict(X))))\n",
    "print(model.imp_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0: Total Loss = 219.04858, Mean Bare Loss = 1.23061\n",
      "\tEpoch 50: Total Loss = 6.51185, Mean Bare Loss = 0.03658\n",
      "\tEpoch 100: Total Loss = 3.35768, Mean Bare Loss = 0.01886\n",
      "\tEpoch 150: Total Loss = 2.18124, Mean Bare Loss = 0.01225\n",
      "\tEpoch 200: Total Loss = 1.55337, Mean Bare Loss = 0.00873\n",
      "\tEpoch 250: Total Loss = 1.17028, Mean Bare Loss = 0.00657\n",
      "\tEpoch 300: Total Loss = 0.91749, Mean Bare Loss = 0.00515\n",
      "\tEpoch 350: Total Loss = 0.74199, Mean Bare Loss = 0.00417\n",
      "\tEpoch 400: Total Loss = 0.61459, Mean Bare Loss = 0.00345\n",
      "\tEpoch 450: Total Loss = 0.51891, Mean Bare Loss = 0.00292\n",
      "\tEpoch 500: Total Loss = 0.44495, Mean Bare Loss = 0.00250\n",
      "\tEpoch 550: Total Loss = 0.38640, Mean Bare Loss = 0.00217\n",
      "\tEpoch 600: Total Loss = 0.33913, Mean Bare Loss = 0.00191\n",
      "\tEpoch 650: Total Loss = 0.30033, Mean Bare Loss = 0.00169\n",
      "\tEpoch 700: Total Loss = 0.26805, Mean Bare Loss = 0.00151\n",
      "\tEpoch 750: Total Loss = 0.24086, Mean Bare Loss = 0.00135\n",
      "\tEpoch 800: Total Loss = 0.21772, Mean Bare Loss = 0.00122\n",
      "\tEpoch 850: Total Loss = 0.19783, Mean Bare Loss = 0.00111\n",
      "\tEpoch 900: Total Loss = 0.18062, Mean Bare Loss = 0.00101\n",
      "\tEpoch 950: Total Loss = 0.16559, Mean Bare Loss = 0.00093\n",
      "\tEpoch 1000: Total Loss = 0.15239, Mean Bare Loss = 0.00086\n",
      "\tEpoch 1050: Total Loss = 0.14073, Mean Bare Loss = 0.00079\n",
      "\tEpoch 1100: Total Loss = 0.13036, Mean Bare Loss = 0.00073\n",
      "\tEpoch 1150: Total Loss = 0.12110, Mean Bare Loss = 0.00068\n",
      "\tEpoch 1200: Total Loss = 0.11279, Mean Bare Loss = 0.00063\n",
      "\tEpoch 1250: Total Loss = 0.10530, Mean Bare Loss = 0.00059\n",
      "\t\tConverged after 1289 epochs. Penalized loss is small.\n",
      "\n",
      "### Intermediate phase 1: Lambda = 10.6255, Nu = 1 ###\n",
      "\tEpoch 0: Total Loss = 697.54987, Mean Bare Loss = 0.00056\n",
      "\tEpoch 50: Total Loss = 224.09950, Mean Bare Loss = 0.02245\n",
      "\tEpoch 100: Total Loss = 94.56412, Mean Bare Loss = 0.12235\n",
      "\tEpoch 150: Total Loss = 72.18799, Mean Bare Loss = 0.13482\n",
      "\tEpoch 200: Total Loss = 62.27052, Mean Bare Loss = 0.12924\n",
      "\tEpoch 250: Total Loss = 56.16653, Mean Bare Loss = 0.11688\n",
      "\tEpoch 300: Total Loss = 54.90414, Mean Bare Loss = 0.11535\n",
      "\tEpoch 350: Total Loss = 54.51011, Mean Bare Loss = 0.11545\n",
      "\tEpoch 400: Total Loss = 53.92448, Mean Bare Loss = 0.11552\n",
      "\tEpoch 450: Total Loss = 53.71825, Mean Bare Loss = 0.11541\n",
      "\tEpoch 500: Total Loss = 53.56444, Mean Bare Loss = 0.11543\n",
      "\tEpoch 550: Total Loss = 53.52129, Mean Bare Loss = 0.11545\n",
      "\tEpoch 600: Total Loss = 53.49477, Mean Bare Loss = 0.11546\n",
      "\t\tConverged after 606 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Intermediate phase 2: Lambda = 19.7543, Nu = 0.7 ###\n",
      "\tEpoch 0: Total Loss = 84.13818, Mean Bare Loss = 0.11546\n",
      "\tEpoch 50: Total Loss = 81.45139, Mean Bare Loss = 0.17851\n",
      "\tEpoch 100: Total Loss = 80.29828, Mean Bare Loss = 0.17736\n",
      "\t\tConverged after 123 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Intermediate phase 3: Lambda = 28.8831, Nu = 0.4 ###\n",
      "\tEpoch 0: Total Loss = 104.55833, Mean Bare Loss = 0.17749\n",
      "\tEpoch 50: Total Loss = 102.82182, Mean Bare Loss = 0.22164\n",
      "\tEpoch 100: Total Loss = 101.67847, Mean Bare Loss = 0.21796\n",
      "\tEpoch 150: Total Loss = 100.59183, Mean Bare Loss = 0.21828\n",
      "\tEpoch 200: Total Loss = 99.81350, Mean Bare Loss = 0.21924\n",
      "\tEpoch 250: Total Loss = 99.29770, Mean Bare Loss = 0.21978\n",
      "\tEpoch 300: Total Loss = 99.07310, Mean Bare Loss = 0.22010\n",
      "\tEpoch 350: Total Loss = 98.87515, Mean Bare Loss = 0.22026\n",
      "\t\tConverged after 368 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Intermediate phase 4: Lambda = 34.7991, Nu = 0.3 ###\n",
      "\tEpoch 0: Total Loss = 111.15611, Mean Bare Loss = 0.22029\n",
      "\tEpoch 50: Total Loss = 109.51982, Mean Bare Loss = 0.24557\n",
      "\tEpoch 100: Total Loss = 108.36707, Mean Bare Loss = 0.24641\n",
      "\tEpoch 150: Total Loss = 107.62456, Mean Bare Loss = 0.24768\n",
      "\tEpoch 200: Total Loss = 107.11568, Mean Bare Loss = 0.24646\n",
      "\tEpoch 250: Total Loss = 106.96559, Mean Bare Loss = 0.24573\n",
      "\tEpoch 300: Total Loss = 106.75092, Mean Bare Loss = 0.24513\n",
      "\t\tConverged after 300 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Intermediate phase 5: Lambda = 37.6349, Nu = 0.2 ###\n",
      "\tEpoch 0: Total Loss = 111.03683, Mean Bare Loss = 0.24513\n",
      "\tEpoch 50: Total Loss = 112.58411, Mean Bare Loss = 0.23698\n",
      "\tEpoch 100: Total Loss = 109.13469, Mean Bare Loss = 0.23303\n",
      "\tEpoch 150: Total Loss = 108.70321, Mean Bare Loss = 0.23098\n",
      "\tEpoch 200: Total Loss = 108.47984, Mean Bare Loss = 0.23014\n",
      "\t\tConverged after 219 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "### Final ISTA Phase ###\n",
      "\t Epoch 0: Loss = 109.63401, Mean Bare Loss = 0.22810, Important Features = (3, [6, 9, 12])\n",
      "\t Epoch 50: Loss = 108.20164, Mean Bare Loss = 0.21031, Important Features = (3, [6, 9, 12])\n",
      "\t Epoch 100: Loss = 107.65010, Mean Bare Loss = 0.23280, Important Features = (3, [6, 9, 12])\n",
      "\t Epoch 150: Loss = 105.56421, Mean Bare Loss = 0.24003, Important Features = (2, [6, 9])\n",
      "\t Epoch 200: Loss = 105.53776, Mean Bare Loss = 0.23663, Important Features = (2, [6, 9])\n",
      "\t\tConverged after 200 epochs. Relative loss change below 1e-06.\n",
      "\n",
      "\tEpoch 0: Total Loss = 42.12002, Mean Bare Loss = 0.23663\n",
      "\tEpoch 50: Total Loss = 35.97464, Mean Bare Loss = 0.20210\n",
      "\tEpoch 100: Total Loss = 32.71331, Mean Bare Loss = 0.18378\n",
      "\tEpoch 150: Total Loss = 30.55776, Mean Bare Loss = 0.17167\n",
      "\tEpoch 200: Total Loss = 29.06824, Mean Bare Loss = 0.16330\n",
      "\tEpoch 250: Total Loss = 28.00925, Mean Bare Loss = 0.15736\n",
      "\tEpoch 300: Total Loss = 27.21541, Mean Bare Loss = 0.15290\n",
      "\tEpoch 350: Total Loss = 26.59590, Mean Bare Loss = 0.14942\n",
      "\tEpoch 400: Total Loss = 26.09699, Mean Bare Loss = 0.14661\n",
      "\tEpoch 450: Total Loss = 25.68503, Mean Bare Loss = 0.14430\n",
      "\tEpoch 500: Total Loss = 25.33782, Mean Bare Loss = 0.14235\n",
      "\tEpoch 550: Total Loss = 25.03999, Mean Bare Loss = 0.14067\n",
      "\tEpoch 600: Total Loss = 24.78048, Mean Bare Loss = 0.13922\n",
      "\tEpoch 650: Total Loss = 24.55149, Mean Bare Loss = 0.13793\n",
      "\tEpoch 700: Total Loss = 24.34616, Mean Bare Loss = 0.13678\n",
      "\tEpoch 750: Total Loss = 24.15996, Mean Bare Loss = 0.13573\n",
      "\tEpoch 800: Total Loss = 23.98892, Mean Bare Loss = 0.13477\n",
      "\tEpoch 850: Total Loss = 23.83024, Mean Bare Loss = 0.13388\n",
      "\tEpoch 900: Total Loss = 23.68208, Mean Bare Loss = 0.13305\n",
      "\tEpoch 950: Total Loss = 23.54324, Mean Bare Loss = 0.13227\n",
      "\tEpoch 1000: Total Loss = 23.41301, Mean Bare Loss = 0.13153\n",
      "\tEpoch 1050: Total Loss = 23.29088, Mean Bare Loss = 0.13085\n",
      "\tEpoch 1100: Total Loss = 23.17651, Mean Bare Loss = 0.13021\n",
      "\tEpoch 1150: Total Loss = 23.06938, Mean Bare Loss = 0.12960\n",
      "\tEpoch 1200: Total Loss = 22.96831, Mean Bare Loss = 0.12904\n",
      "\tEpoch 1250: Total Loss = 22.86653, Mean Bare Loss = 0.12846\n",
      "\tEpoch 1300: Total Loss = 22.76079, Mean Bare Loss = 0.12787\n",
      "\tEpoch 1350: Total Loss = 22.65125, Mean Bare Loss = 0.12725\n",
      "\tEpoch 1400: Total Loss = 22.54285, Mean Bare Loss = 0.12665\n",
      "\tEpoch 1450: Total Loss = 22.45027, Mean Bare Loss = 0.12613\n",
      "\tEpoch 1500: Total Loss = 22.37023, Mean Bare Loss = 0.12568\n",
      "\tEpoch 1550: Total Loss = 22.29510, Mean Bare Loss = 0.12525\n",
      "\tEpoch 1600: Total Loss = 22.22610, Mean Bare Loss = 0.12487\n",
      "\tEpoch 1650: Total Loss = 22.16135, Mean Bare Loss = 0.12450\n",
      "\tEpoch 1700: Total Loss = 22.10052, Mean Bare Loss = 0.12416\n",
      "\tEpoch 1750: Total Loss = 22.04247, Mean Bare Loss = 0.12383\n",
      "\tEpoch 1800: Total Loss = 21.98754, Mean Bare Loss = 0.12353\n",
      "\tEpoch 1850: Total Loss = 21.93523, Mean Bare Loss = 0.12323\n",
      "\tEpoch 1900: Total Loss = 21.88532, Mean Bare Loss = 0.12295\n",
      "\tEpoch 1950: Total Loss = 21.83752, Mean Bare Loss = 0.12268\n",
      "\tEpoch 2000: Total Loss = 21.79170, Mean Bare Loss = 0.12243\n",
      "\tEpoch 2050: Total Loss = 21.74770, Mean Bare Loss = 0.12218\n",
      "\tEpoch 2100: Total Loss = 21.70538, Mean Bare Loss = 0.12194\n",
      "\tEpoch 2150: Total Loss = 21.66465, Mean Bare Loss = 0.12171\n",
      "\tEpoch 2200: Total Loss = 21.62527, Mean Bare Loss = 0.12149\n",
      "\tEpoch 2250: Total Loss = 21.58726, Mean Bare Loss = 0.12128\n",
      "\tEpoch 2300: Total Loss = 21.55094, Mean Bare Loss = 0.12107\n",
      "\tEpoch 2350: Total Loss = 21.51493, Mean Bare Loss = 0.12087\n",
      "\tEpoch 2400: Total Loss = 21.48043, Mean Bare Loss = 0.12068\n",
      "\tEpoch 2450: Total Loss = 21.44696, Mean Bare Loss = 0.12049\n",
      "\tEpoch 2500: Total Loss = 21.41451, Mean Bare Loss = 0.12031\n",
      "\tEpoch 2550: Total Loss = 21.38289, Mean Bare Loss = 0.12013\n",
      "\tEpoch 2600: Total Loss = 21.35218, Mean Bare Loss = 0.11996\n",
      "\tEpoch 2650: Total Loss = 21.32303, Mean Bare Loss = 0.11979\n",
      "\tEpoch 2700: Total Loss = 21.29320, Mean Bare Loss = 0.11962\n",
      "\tEpoch 2750: Total Loss = 21.26484, Mean Bare Loss = 0.11947\n",
      "\tEpoch 2800: Total Loss = 21.23719, Mean Bare Loss = 0.11931\n",
      "\tEpoch 2850: Total Loss = 21.21025, Mean Bare Loss = 0.11916\n",
      "\tEpoch 2900: Total Loss = 21.18394, Mean Bare Loss = 0.11901\n",
      "\tEpoch 2950: Total Loss = 21.15825, Mean Bare Loss = 0.11887\n",
      "\tEpoch 3000: Total Loss = 21.13317, Mean Bare Loss = 0.11873\n",
      "\tEpoch 3050: Total Loss = 21.10880, Mean Bare Loss = 0.11859\n",
      "\tEpoch 3100: Total Loss = 21.08475, Mean Bare Loss = 0.11845\n",
      "\tEpoch 3150: Total Loss = 21.06134, Mean Bare Loss = 0.11832\n",
      "\tEpoch 3200: Total Loss = 21.03845, Mean Bare Loss = 0.11819\n",
      "\tEpoch 3250: Total Loss = 21.01620, Mean Bare Loss = 0.11807\n",
      "\tEpoch 3300: Total Loss = 20.99418, Mean Bare Loss = 0.11794\n",
      "\tEpoch 3350: Total Loss = 20.97275, Mean Bare Loss = 0.11782\n",
      "\tEpoch 3400: Total Loss = 20.95177, Mean Bare Loss = 0.11771\n",
      "\tEpoch 3450: Total Loss = 20.93134, Mean Bare Loss = 0.11759\n",
      "\tEpoch 3500: Total Loss = 20.91115, Mean Bare Loss = 0.11748\n",
      "\tEpoch 3550: Total Loss = 20.89147, Mean Bare Loss = 0.11737\n",
      "\tEpoch 3600: Total Loss = 20.87218, Mean Bare Loss = 0.11726\n",
      "\tEpoch 3650: Total Loss = 20.85365, Mean Bare Loss = 0.11716\n",
      "\tEpoch 3700: Total Loss = 20.83478, Mean Bare Loss = 0.11705\n",
      "\tEpoch 3750: Total Loss = 20.81664, Mean Bare Loss = 0.11695\n",
      "\tEpoch 3800: Total Loss = 20.79943, Mean Bare Loss = 0.11685\n",
      "\tEpoch 3850: Total Loss = 20.79270, Mean Bare Loss = 0.11681\n",
      "\t\tConverged after 3874 epochs. Relative loss change below 1e-10.\n",
      "\n",
      "0.949438202247191\n",
      "(2, [6, 9])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X,y = load_wine(return_X_y=True)\n",
    "model = harderLASSOClassifier()\n",
    "model.fit(X,y,1)\n",
    "print(accuracy_score(model.predict(X), y))\n",
    "print(model.imp_feat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
